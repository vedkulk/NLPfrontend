import { HfInference } from "@huggingface/inference";
import { Pinecone } from "@pinecone-database/pinecone";

const hf = new HfInference(process.env.HUGGING_FACE_TOKEN);

export async function queryPineconeVectorStore(
  client: Pinecone,
  indexName: string,
  namespace: string,
  query: string
): Promise<string> {
  try {
    console.log("üîπ Sending query to Hugging Face...");
    
    // Ensure the HF request does not hang
    const hfOutput = await hf.featureExtraction({
      model: "intfloat/multilingual-e5-large",
      inputs: query,
    });

    if (!hfOutput || !Array.isArray(hfOutput)) {
      throw new Error("‚ùå Hugging Face output is invalid.");
    }

    const queryEmbedding = Array.from(hfOutput);
    console.log("‚úÖ Hugging Face response received. Embedding generated.");

    console.log("üîπ Querying Pinecone database...");
    const index = client.Index(indexName);
    const queryResponse = await index.namespace(namespace).query({
      topK: 5,
      vector: queryEmbedding as any,
      includeMetadata: true,
      includeValues: false,
    });

    if (!queryResponse || !queryResponse.matches) {
      console.error("‚ùå Pinecone query returned no valid matches.");
      return "<nomatches>";
    }

    if (queryResponse.matches.length > 0) {
      const concatenatedRetrievals = queryResponse.matches
        .map((match, index) => `\nLegal Findings ${index + 1}: \n ${match.metadata?.chunk}`)
        .join(". \n\n");

      console.log("‚úÖ Retrieved legal findings:");
      console.log(concatenatedRetrievals);

      // Ensure the data stream is properly closed
      if (queryResponse.stream) {
        console.log("üîπ Closing data stream...");
        queryResponse.stream.close(); // <-- This ensures the stream is properly closed
      }

      return concatenatedRetrievals;
    } else {
      console.log("‚ùå No matches found.");
      return "<nomatches>";
    }
  } catch (error) {
    console.error("‚ùå Error in queryPineconeVectorStore:", error);
    return "<error>";
  }
}
